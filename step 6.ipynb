{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537d905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/04 10:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/04 10:10:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/04 10:10:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b607e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------------------+---------------------+--------------------+-----------+------------------+\n",
      "|    Country|Gender|Demographics Question|Demographics Response|            Question|Survey Year|             Value|\n",
      "+-----------+------+---------------------+---------------------+--------------------+-----------+------------------+\n",
      "|Afghanistan|     F|            Education|               Higher|... if she burns ...| 01/01/2015|3.1780497764304645|\n",
      "|Afghanistan|     F|            Education|            Secondary|... if she burns ...| 01/01/2015|3.7013510788987767|\n",
      "|Afghanistan|     F|            Education|              Primary|... if she burns ...| 01/01/2015|3.7148351498733914|\n",
      "|Afghanistan|     F|       Marital status| Widowed, divorced...|... if she burns ...| 01/01/2015|3.7148351498733914|\n",
      "|Afghanistan|     F|           Employment|    Employed for kind|... if she burns ...| 01/01/2015| 4.123105625617661|\n",
      "|Afghanistan|     F|                  Age|                15-24|... if she burns ...| 01/01/2015| 4.159326776902789|\n",
      "|Afghanistan|     F|           Employment|           Unemployed|... if she burns ...| 01/01/2015| 4.242640687119285|\n",
      "|Afghanistan|     F|            Residence|                Rural|... if she burns ...| 01/01/2015| 4.254409522068806|\n",
      "|Afghanistan|     F|                  Age|                25-34|... if she burns ...| 01/01/2015| 4.266145890958191|\n",
      "|Afghanistan|     F|       Marital status| Married or living...|... if she burns ...| 01/01/2015| 4.277849838068249|\n",
      "|Afghanistan|     F|            Residence|                Urban|... if she burns ...| 01/01/2015| 4.277849838068249|\n",
      "|Afghanistan|     F|                  Age|                35-49|... if she burns ...| 01/01/2015| 4.335896589756327|\n",
      "|Afghanistan|     F|            Education|         No education|... if she burns ...| 01/01/2015| 4.370354720325311|\n",
      "|Afghanistan|     F|           Employment|    Employed for cash|... if she burns ...| 01/01/2015|  4.56070161675378|\n",
      "|Afghanistan|     M|            Education|               Higher|... if she burns ...| 01/01/2015|2.1213203435596424|\n",
      "|Afghanistan|     M|            Residence|                Urban|... if she burns ...| 01/01/2015|2.1447610367200745|\n",
      "|Afghanistan|     M|           Employment|           Unemployed|... if she burns ...| 01/01/2015|  2.28035080837689|\n",
      "|Afghanistan|     M|            Education|              Primary|... if she burns ...| 01/01/2015|2.5099801175975207|\n",
      "|Afghanistan|     M|       Marital status| Widowed, divorced...|... if she burns ...| 01/01/2015|2.5099801175975207|\n",
      "|Afghanistan|     M|            Education|            Secondary|... if she burns ...| 01/01/2015|2.6645825009994657|\n",
      "+-----------+------+---------------------+---------------------+--------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('dataset1-1/part-00000-08c7db4a-479f-4c60-9ae4-4ae109f8ec54-c000.csv', header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee90567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 KB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.22.4)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.1 threadpoolctl-3.2.0\n",
      "RandomForestRegressor RMSE: 0.5735223343369803\n",
      "MLPRegressor RMSE: 0.2598672509816248\n",
      "LinearRegression RMSE: 0.8082320470140498\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "from pyspark.sql.functions import col\n",
    "import seaborn as sns\n",
    "from pyspark.sql.types import FloatType,IntegerType\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = df.withColumn(\"Value\", col(\"Value\").cast(FloatType()))\n",
    "df1 = df.toPandas()\n",
    "\n",
    "X_Country = pd.get_dummies(df1['Country'], drop_first=True, dtype=int)\n",
    "X_Gender = pd.get_dummies(df1['Gender'], drop_first=True, dtype=int)\n",
    "X_dq = pd.get_dummies(df1['Demographics Question'], drop_first=True, dtype=int)\n",
    "X_dr = pd.get_dummies(df1['Demographics Response'], drop_first=True, dtype=int)\n",
    "X_Question = pd.get_dummies(df1['Question'], drop_first=True, dtype=int)\n",
    "X_sy = pd.get_dummies(df1['Survey Year'], drop_first=True, dtype=int)\n",
    "\n",
    "df_encoded = pd.concat([df1, X_Country, X_Gender,X_dq,X_dr,X_Question,X_sy], axis=1)\n",
    "\n",
    "df_encoded.drop(['Country', 'Gender','Demographics Question','Demographics Response','Question','Survey Year'], axis=1, inplace=True)\n",
    "\n",
    "y = df_encoded['Value']\n",
    "\n",
    "X = df_encoded.drop('Value', axis=1)\n",
    "X = sm.add_constant(X)  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# MLPRegressor\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42)\n",
    "nn_model.fit(X_train, y_train)\n",
    "nn_predictions = nn_model.predict(X_test)\n",
    "\n",
    "# LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "\n",
    "\n",
    "rf_rmse = mean_squared_error(y_test, rf_predictions, squared=False)\n",
    "nn_rmse = mean_squared_error(y_test, nn_predictions, squared=False)\n",
    "lr_rmse = mean_squared_error(y_test, lr_predictions, squared=False)\n",
    "\n",
    "\n",
    "print(f\"RandomForestRegressor RMSE: {rf_rmse}\")\n",
    "print(f\"MLPRegressor RMSE: {nn_rmse}\")\n",
    "print(f\"LinearRegression RMSE: {lr_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9021fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+------------------+-------------------+--------------------+\n",
      "|summary|FECHA HECHO|GENERO|          CANTIDAD|    FECHA HECHO_STR|         Weapon Used|\n",
      "+-------+-----------+------+------------------+-------------------+--------------------+\n",
      "|  count|     466679|466679|            466679|             466679|              466679|\n",
      "|   mean|       null|  null|1.5416313997415783|               null|                null|\n",
      "| stddev|       null|  null|1.6656822733680472|               null|                null|\n",
      "|    min| 2015-01-01|     F|                 1|2015-01-01 00:00:00|No Weapon or Subs...|\n",
      "|    max| 2023-02-28|     M|                 9|2023-02-28 00:00:00|Using weapons and...|\n",
      "+-------+-----------+------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+------+--------------------+-----------------------+\n",
      "|summary|FECHA HECHO|          CANTIDAD|GENERO|         Weapon Used|Total domestic violence|\n",
      "+-------+-----------+------------------+------+--------------------+-----------------------+\n",
      "|  count|      56832|             56832| 56832|               56832|                  56832|\n",
      "|   mean|       null|3.9417053772522523|  null|                null|      8.211553350225225|\n",
      "| stddev|       null|3.6173151565087447|  null|                null|     15.789425843555712|\n",
      "|    min| 2015-01-01|                 1|     F|No Weapon or Subs...|                    1.0|\n",
      "|    max| 2023-02-28|                 9|     M|Using weapons and...|                   99.0|\n",
      "+-------+-----------+------------------+------+--------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('dataset4/part-00000-e8fbff2b-4af5-4659-8ee9-feb16b3e44b4-c000.csv', header=True)\n",
    "df2.describe().show()\n",
    "\n",
    "df3 = spark.read.csv('dataset3/part-00000-97be97e5-9192-457c-86ac-38f5a8a3b2aa-c000.csv', header=True)\n",
    "df3.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f321772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 124:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.43348958768511714\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3 = df3.withColumn(\"CANTIDAD\", col(\"CANTIDAD\").cast(IntegerType()))\n",
    "data=df3\n",
    "\n",
    "gender_indexer = StringIndexer(inputCol=\"GENERO\", outputCol=\"GENEROIndex\")\n",
    "gender_encoder = OneHotEncoder(inputCol=\"GENEROIndex\", outputCol=\"GENEROVec\")\n",
    "wp_indexer = StringIndexer(inputCol=\"Weapon Used\", outputCol=\"WeaponUsedIndex\")\n",
    "wp_encoder = OneHotEncoder(inputCol=\"WeaponUsedIndex\", outputCol=\"WeaponUsedVec\")\n",
    "\n",
    "preprocessing_stages = [gender_indexer, gender_encoder, wp_indexer, wp_encoder]\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "preprocessing_model = preprocessing_pipeline.fit(data)\n",
    "preprocessed_data = preprocessing_model.transform(data)\n",
    "\n",
    "feature_cols = [\"GENEROVec\", \"CANTIDAD\", \"WeaponUsedVec\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(preprocessed_data)\n",
    "\n",
    "\n",
    "# create K-Means model\n",
    "kmeans = KMeans(k=4,featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans.fit(assembled_data)\n",
    "clustered_data = kmeans_model.transform(assembled_data)\n",
    "\n",
    "#silhouette_score\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"features\")\n",
    "silhouette_score = evaluator.evaluate(clustered_data)\n",
    "print(\"Silhouette Score:\", silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facf74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.5894037254447393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df2 = df2.withColumn(\"CANTIDAD\", col(\"CANTIDAD\").cast(IntegerType()))\n",
    "data=df2\n",
    "\n",
    "gender_indexer = StringIndexer(inputCol=\"GENERO\", outputCol=\"GENEROIndex\")\n",
    "gender_encoder = OneHotEncoder(inputCol=\"GENEROIndex\", outputCol=\"GENEROVec\")\n",
    "wp_indexer = StringIndexer(inputCol=\"Weapon Used\", outputCol=\"WeaponUsedIndex\")\n",
    "wp_encoder = OneHotEncoder(inputCol=\"WeaponUsedIndex\", outputCol=\"WeaponUsedVec\")\n",
    "\n",
    "preprocessing_stages = [gender_indexer, gender_encoder, wp_indexer, wp_encoder]\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "preprocessing_model = preprocessing_pipeline.fit(data)\n",
    "preprocessed_data = preprocessing_model.transform(data)\n",
    "\n",
    "feature_cols = [\"GENEROVec\", \"CANTIDAD\", \"WeaponUsedVec\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(preprocessed_data)\n",
    "\n",
    "\n",
    "# create K-Means model\n",
    "kmeans = KMeans(k=4,featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans.fit(assembled_data)\n",
    "clustered_data = kmeans_model.transform(assembled_data)\n",
    "\n",
    "#silhouette_score\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"features\")\n",
    "silhouette_score = evaluator.evaluate(clustered_data)\n",
    "print(\"Silhouette Score:\", silhouette_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b0145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
